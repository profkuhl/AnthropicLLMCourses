{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Precognition (Thinking Step by Step)\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "API_KEY = os.environ['ANTHROPIC_API_KEY']\n",
    "MODEL_NAME = \"claude-3-haiku-20240307\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/, https://ryankuhl:****@pypi.tataridev.com/simple/\n",
      "Requirement already satisfied: anthropic in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (0.45.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/ryankuhl/Documents/AnthropicLLMCourses/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic\n",
    "\n",
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import anthropic\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "def get_completion(prompt: str, system_prompt=\"\", prefill=\"\"):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt},\n",
    "          {\"role\": \"assistant\", \"content\": prefill}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to **think through your answer first**. \n",
    "\n",
    "Guess what? Claude is the same way.\n",
    "\n",
    "**Giving Claude time to think step by step sometimes makes Claude more accurate**, particularly for complex tasks. However, **thinking only counts when it's out loud**. You cannot ask Claude to think but output only the answer - in this case, no thinking has actually occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "In the prompt below, it's clear to a human reader that the second sentence belies the first. But **Claude takes the word \"unrelated\" too literally**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this movie review is positive.\n",
      "\n",
      "The first part of the review states that the movie \"blew my mind with its freshness and originality\", which indicates a very positive reaction to the film.\n",
      "\n",
      "The second part about living under a rock since 1900 is likely a humorous or sarcastic remark, but it does not negate the overall positive sentiment expressed in the first part of the review.\n",
      "\n",
      "So based on the language used, this movie review has a positive sentiment, despite the somewhat tongue-in-cheek final statement.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"\"\"Is this movie review sentiment positive or negative?\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve Claude's response, let's **allow Claude to think things out first before answering**. We do that by literally spelling out the steps that Claude should take in order to process and think through its task. Along with a dash of role prompting, this empowers Claude to understand the review more deeply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<positive-argument>\n",
      "- The review suggests that the movie was highly innovative and unique, describing it as \"fresh and original\" and blowing the reviewer's mind.\n",
      "- The reviewer's comment about living under a rock since 1900 could be interpreted as a self-deprecating joke, implying that the reviewer is not up-to-date with the latest trends and was pleasantly surprised by the movie's originality.\n",
      "</positive-argument>\n",
      "\n",
      "<negative-argument>\n",
      "- The reviewer's comment about living under a rock since 1900 could be seen as a sarcastic remark, implying that the movie's \"freshness and originality\" are not actually new or innovative, but rather something the reviewer should have been aware of for a long time.\n",
      "- The review could be interpreted as backhanded or dismissive, with the reviewer suggesting that the movie is only impressive to someone who has been completely disconnected from popular culture for over a century.\n",
      "</negative-argument>\n",
      "\n",
      "Based on the arguments presented, the sentiment of the review is ambiguous. The positive and negative interpretations are both plausible, and the reviewer's comment about living under a rock could be read in either a positive or negative light. Without additional context, it is difficult to determine the overall sentiment of the review.\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"You are a savvy reader of movie reviews.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment positive or negative? First, write the best arguments for each side in <positive-argument> and <negative-argument> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Claude is sometimes sensitive to ordering**. This example is on the frontier of Claude's ability to understand nuanced text, and when we swap the order of the arguments from the previous example so that negative is first and positive is second, this changes Claude's overall assessment to positive.\n",
    "\n",
    "In most situations (but not all, confusingly enough), **Claude is more likely to choose the second of two options**, possibly because in its training data from the web, second options were more likely to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment negative or positive? First write the best arguments for each side in <negative-argument> and <positive-argument> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Letting Claude think can shift Claude's answer from incorrect to correct**. It's that simple in many cases where Claude makes mistakes!\n",
    "\n",
    "Let's go through an example where Claude's answer is incorrect to see how asking Claude to think can fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix this by asking Claude to think step by step, this time in `<brainstorm>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in <brainstorm> tags, then give your answer.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the [**Example Playground**](#example-playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "- [Exercise 6.1 - Classifying Emails](#exercise-61---classifying-emails)\n",
    "- [Exercise 6.2 - Email Classification Formatting](#exercise-62---email-classification-formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1 - Classifying Emails\n",
    "In this exercise, we'll be instructing Claude to sort emails into the following categories:\t\t\t\t\t\t\t\t\t\t\n",
    "- (A) Pre-sale question\n",
    "- (B) Broken or defective item\n",
    "- (C) Billing question\n",
    "- (D) Other (please explain)\n",
    "\n",
    "For the first part of the exercise, change the `PROMPT` to **make Claude output the correct classification and ONLY the classification**. Your answer needs to **include the letter (A - D) of the correct choice, with the parentheses, as well as the name of the category**.\n",
    "\n",
    "Refer to the comments beside each email in the `EMAILS` list to know which category that email should be classified under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = \"\"\"Please classify this email as either green or blue: {email}\"\"\"\n",
    "\n",
    "# Prefill for Claude's response, if any\n",
    "PREFILL = \"\"\n",
    "\n",
    "# Variable content stored as a list\n",
    "EMAILS = [\n",
    "    \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\", # (B) Broken or defective item\n",
    "    \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\", # (A) Pre-sale question OR (D) Other (please explain)\n",
    "    \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\", # (C) Billing question\n",
    "    \"How did I get here I am not good with computer.  Halp.\" # (D) Other (please explain)\n",
    "]\n",
    "\n",
    "# Correct categorizations stored as a list of lists to accommodate the possibility of multiple correct categorizations per email\n",
    "ANSWERS = [\n",
    "    [\"B\"],\n",
    "    [\"A\",\"D\"],\n",
    "    [\"C\"],\n",
    "    [\"D\"]\n",
    "]\n",
    "\n",
    "# Dictionary of string values for each category to be used for regex grading\n",
    "REGEX_CATEGORIES = {\n",
    "    \"A\": \"A\\) P\",\n",
    "    \"B\": \"B\\) B\",\n",
    "    \"C\": \"C\\) B\",\n",
    "    \"D\": \"D\\) O\"\n",
    "}\n",
    "\n",
    "# Iterate through list of emails\n",
    "for i,email in enumerate(EMAILS):\n",
    "\n",
    "    # Substitute the email text into the email placeholder variable\n",
    "    formatted_prompt = PROMPT.format(email=email)\n",
    "\n",
    "    # Get Claude's response\n",
    "    response = get_completion(formatted_prompt, prefill=PREFILL)\n",
    "\n",
    "    # Grade Claude's response\n",
    "    grade = any([bool(re.search(REGEX_CATEGORIES[ans], response)) for ans in ANSWERS[i]])\n",
    "\n",
    "    # Print Claude's response\n",
    "    print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "    print(\"USER TURN\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"\\nASSISTANT TURN\")\n",
    "    print(PREFILL)\n",
    "    print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "    print(response)\n",
    "    print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "    print(\"This exercise has been correctly solved:\", grade, \"\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_6_1_hint; print(exercise_6_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still stuck? Run the cell below for an example solution.\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_6_1_solution; print(exercise_6_1_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2 - Email Classification Formatting\n",
    "In this exercise, we're going to refine the output of the above prompt to yield an answer formatted exactly how we want it. \n",
    "\n",
    "Use your favorite output formatting technique to make Claude wrap JUST the letter of the correct classification in `<answer></answer>` tags. For instance, the answer to the first email should contain the exact string `<answer>B</answer>`.\n",
    "\n",
    "Refer to the comments beside each email in the `EMAILS` list if you forget which letter category is correct for each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template with a placeholder for the variable content\n",
    "PROMPT = \"\"\"Please classify this email as either green or blue: {email}\"\"\"\n",
    "\n",
    "# Prefill for Claude's response, if any\n",
    "PREFILL = \"\"\n",
    "\n",
    "# Variable content stored as a list\n",
    "EMAILS = [\n",
    "    \"Hi -- My Mixmaster4000 is producing a strange noise when I operate it. It also smells a bit smoky and plasticky, like burning electronics.  I need a replacement.\", # (B) Broken or defective item\n",
    "    \"Can I use my Mixmaster 4000 to mix paint, or is it only meant for mixing food?\", # (A) Pre-sale question OR (D) Other (please explain)\n",
    "    \"I HAVE BEEN WAITING 4 MONTHS FOR MY MONTHLY CHARGES TO END AFTER CANCELLING!!  WTF IS GOING ON???\", # (C) Billing question\n",
    "    \"How did I get here I am not good with computer.  Halp.\" # (D) Other (please explain)\n",
    "]\n",
    "\n",
    "# Correct categorizations stored as a list of lists to accommodate the possibility of multiple correct categorizations per email\n",
    "ANSWERS = [\n",
    "    [\"B\"],\n",
    "    [\"A\",\"D\"],\n",
    "    [\"C\"],\n",
    "    [\"D\"]\n",
    "]\n",
    "\n",
    "# Dictionary of string values for each category to be used for regex grading\n",
    "REGEX_CATEGORIES = {\n",
    "    \"A\": \"<answer>A</answer>\",\n",
    "    \"B\": \"<answer>B</answer>\",\n",
    "    \"C\": \"<answer>C</answer>\",\n",
    "    \"D\": \"<answer>D</answer>\"\n",
    "}\n",
    "\n",
    "# Iterate through list of emails\n",
    "for i,email in enumerate(EMAILS):\n",
    "\n",
    "    # Substitute the email text into the email placeholder variable\n",
    "    formatted_prompt = PROMPT.format(email=email)\n",
    "\n",
    "    # Get Claude's response\n",
    "    response = get_completion(formatted_prompt, prefill=PREFILL)\n",
    "\n",
    "    # Grade Claude's response\n",
    "    grade = any([bool(re.search(REGEX_CATEGORIES[ans], response)) for ans in ANSWERS[i]])\n",
    "\n",
    "    # Print Claude's response\n",
    "    print(\"--------------------------- Full prompt with variable substutions ---------------------------\")\n",
    "    print(\"USER TURN\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"\\nASSISTANT TURN\")\n",
    "    print(PREFILL)\n",
    "    print(\"\\n------------------------------------- Claude's response -------------------------------------\")\n",
    "    print(response)\n",
    "    print(\"\\n------------------------------------------ GRADING ------------------------------------------\")\n",
    "    print(\"This exercise has been correctly solved:\", grade, \"\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_6_2_hint; print(exercise_6_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Playground\n",
    "\n",
    "This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"\"\"Is this movie review sentiment positive or negative?\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since the year 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"You are a savvy reader of movie reviews.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment positive or negative? First, write the best arguments for each side in <positive-argument> and <negative-argument> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. In totally unrelated news, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"\"\"Is this review sentiment negative or positive? First write the best arguments for each side in <negative-argument> and <positive-argument> XML tags, then answer.\n",
    "\n",
    "This movie blew my mind with its freshness and originality. Unrelatedly, I have been living under a rock since 1900.\"\"\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Name a famous movie starring an actor who was born in the year 1956. First brainstorm about some actors and their birth years in <brainstorm> tags, then give your answer.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
